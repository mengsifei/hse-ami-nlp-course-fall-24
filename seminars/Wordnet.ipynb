{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bvfb0plD6AMC"
      },
      "source": [
        "# Wordnet sandbox\n",
        "\n",
        "\n",
        "## Preface\n",
        "\n",
        "This tutorial illustrates the use of Wordnet for the types of exploration to be conducted in the [Dante’s _Inferno_](http://dante.obdurodon.org) and [Victorian ghost stories](http://ghost.obdurodon.org) research projects that were part of a [Computational methods in the humanities](http://dh.obdurodon.org) course in the autumn 2016 academic semester. Thanks to Na-Rae Han for discussion and suggestions.\n",
        "\n",
        "Students completing [Computational methods in the humanities](http://dh.obdurodon.org) to satisfy the “methods” requirement for the Linguistics major need to perform some linguistic tasks with their data, and Wordnet is one way to do that. Below, after an introduction to how Wordnet works, we describe how to add Wordnet-related markup to your XML and how to use that markup to explore your data. You do not need to add Wordnet-related markup to all of your data (which would not be feasible within the context of a semester-long course because some of the work must be performed manually and your documents may be long), but you should do enough of it to be able to experiment a bit with how it works. You also do not have to perform all of the tasks we describe below (which also would not be feasible in the available time); pick one or two that sound interesting and see what you’re able to learn about your documents by implementing them. Ask your instructors should you have any questions about either the content of this tutorial (that is, about how to use Wordnet) or the scope of the assignment.\n",
        "\n",
        "**tl;dr:** Use Wordnet as described below to add semantic markup to some (not all) of your data. Then perform some (not all) of the tasks below to explore how meaning is represented in your texts.\n",
        "\n",
        "## Introduction\n",
        "\n",
        "In Real Life you’ll export the words you care about from your XML using XSLT and then read the list into your Python program, but to start, let’s concentrate on learning how Wordet works. We’re writing this tutorial in the **Jupyter notebook** interface, which allows us to break up the code into pieces that are interspersed with discussion. Because the code is fragmented, in order to run the statements at the bottom of the page you need to have run at least some of the ones at the top. For example, we import Wordnet at the beginning with `from nltk.corpus import wordnet as wn`, and later code depends on our having done that. This means that if you copy and try to run something below without having done the import, you’ll throw an error. We also create some variables near the top that we use below without redeclaring them. You don’t need to use Jupyter notebook for your own development; we’ve used it here because the combination of code cells and text cells is convenient for tutorial purposes.\n",
        "\n",
        "**tl;dr:** Run the code from the top of this notebook to the bottom, and not just in a single cell.\n",
        "\n",
        "## How Wordnet is organized\n",
        "\n",
        "Wordnet is a hierarchical organization of units of meaning, called **synsets**. Synsets are represented in texts by **words**, and a combination of a **lexeme** (represented by the dictionary form of a word) with a specific synset is called a **lemma**. Synsets are identified within Wordnet by three dot-separated parts:\n",
        "\n",
        "1. A representative word, that is, a word that conveys the meaning of the synset. This representative word may not be the only word that conveys that meaning, and it may also be able to convey other meanings. We’ll see below that that the lexeme ‘ghost’ can represent several different meanings (that is, is associated with multiple synsets), and that each of those meanings can alternatively be conveyed by lexemes other than “ghost”.\n",
        "1. A part of speech (POS) identifier, like “n” for ‘noun’ or “v” for ‘verb’.\n",
        "1. A two-digit number that distinguishes different synsets that may have the same head word and the same POS, but that convey different meaning. For example, the synsets 'ghost.n.01' and 'ghost.n.02' are two different nominal meanings that can be expressed by the lexeme “ghost.”\n",
        "\n",
        "### Exploring synsets\n",
        "\n",
        "There’s a lot more organization within Wordnet, but for the purpose of this tutorial we’re going to stick to the information conveyed through synsets. Let’s explore that with the synset 'koala.n.01', which is a noun that represents a particular arboreal Australian marsupial. Here’s how it looks when we ask Python about it:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BBIvFW4T6AMJ",
        "outputId": "d49c4237-c7f9-45af-ad3d-94df848e6748"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Synset('koala.n.01')"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "from nltk.corpus import wordnet as wn # import Wordnet and call it just “wn” for brevity\n",
        "wn.synset('koala.n.01')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Gj3LBqm6AMM"
      },
      "source": [
        "The output above tells us that the synset 'koala.n.01' is a synset that Wordnet calls 'koala.n.01'. That tautology isn’t very useful, so the only point of the code snippet above is to determine whether such a synset exists. If it doesn’t, we’ll get an error. You can test this by running the cell below, which will raise an error because there is no 'koala.n.02' synset in Wordnet (your error message may differ from ours):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 437
        },
        "id": "OKwi7Tln6AMO",
        "outputId": "2e276133-aef6-4d5b-bc08-3bec1c1b776f"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "WordNetError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/nltk/corpus/reader/wordnet.py\u001b[0m in \u001b[0;36msynset\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1438\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1439\u001b[0;31m             \u001b[0moffset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lemma_pos_offset_map\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlemma\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpos\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msynset_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1440\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIndexError\u001b[0m: list index out of range",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mWordNetError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-b308c44132cd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mwn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msynset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'koala.n.02'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/nltk/corpus/reader/wordnet.py\u001b[0m in \u001b[0;36msynset\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1448\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1449\u001b[0m                 \u001b[0mtup\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlemma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpos\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_senses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"senses\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1450\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mWordNetError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mtup\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1451\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1452\u001b[0m         \u001b[0;31m# load synset information from the appropriate file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mWordNetError\u001b[0m: lemma 'koala' with part of speech 'n' has only 1 sense"
          ]
        }
      ],
      "source": [
        "wn.synset('koala.n.02')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8f6SgPay6AMQ"
      },
      "source": [
        "How can we know that there is a 'koala.n.01' synset but not 'koala.n.02' synset without having to ask for the latter and raising an error? We can ask Wordnet to tell us about all of the synsets associated with the word ‘koala’ by using the `wn.synsets()` function:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vJEJUkrD6AMS",
        "outputId": "daad739c-112e-40b6-f344-3fdba7c5679d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Synset('koala.n.01')]"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "wn.synsets('koala')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "84yplJOG6AMU"
      },
      "source": [
        "The preceding code tells us that there is exactly one synset associated with the word ‘koala’, and that the synset is called 'koala.n.01'."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CM6fYYBc6AMV"
      },
      "source": [
        "### Getting the definition of a synset\n",
        "\n",
        "Synsets are units of meaning, and we can ask for a definition of a synset by using the `.definition()` method:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "uI2Y6GKa6AMY",
        "outputId": "cc88246d-e2d4-47c4-ede8-469b0d5f9f5f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'sluggish tailless Australian arboreal marsupial with grey furry ears and coat; feeds on eucalyptus leaves and bark'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "wn.synset('koala.n.01').definition()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qv_yHh8q6AMc"
      },
      "source": [
        "### Getting the lexemes associated with a synset\n",
        "\n",
        "As we write above, synsets, as units of meaning, are represented in a text by lexemes, and the combination of a synset (a meaning) plus a lexeme (a word) is called a **lemma**. We can get the lemmata for a particular synset by asking for them with the `.lemmas()` method:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0jTKBuZG6AMf",
        "outputId": "6cc437d8-3e32-4636-d36f-f240c6186053"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Lemma('koala.n.01.koala'),\n",
              " Lemma('koala.n.01.koala_bear'),\n",
              " Lemma('koala.n.01.kangaroo_bear'),\n",
              " Lemma('koala.n.01.native_bear'),\n",
              " Lemma('koala.n.01.Phascolarctos_cinereus')]"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "wn.synset('koala.n.01').lemmas()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EG7DMRRY6AMj"
      },
      "source": [
        "Note that a lemma like 'koala.n.01.koala' combines the synset representation (“koala.n.01”) with a lexeme that expresses that meaning (“koala”). You can get just the lexical part, without the synset prefix, by applying the `name()` method to a lemma. Here we ask for the first (zeroeth in Python enumeration) lemma associated with our synset and return just its name:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "5oDvpXuQ6AMl",
        "outputId": "e677b84b-ac51-4976-f37e-cad0d8611a19"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'koala'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "wn.synset('koala.n.01').lemmas()[0].name()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Klc8w1VL6AMn"
      },
      "source": [
        "### What about inflected forms?\n",
        "\n",
        "As noted above, we can identify all of the synsets associated with a word by using the `wn.synsets()` function:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jys02N5k6AMo",
        "outputId": "4166f5e6-8f5e-49dc-b4d9-d503916aaf69"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Synset('koala.n.01')]"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "wn.synsets('koala')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dI_F-qQS6AMq"
      },
      "source": [
        "The word that we use as an argument to the `wn.synsets()` function doesn’t have to be the dictionary form, which for nouns is a typically a singular. We’ll get the same result if we ask for the synsets associated with the plural:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W6N15_MB6AMt",
        "outputId": "6874af98-ff0d-441d-aede-cdfdc96ee757"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Synset('koala.n.01')]"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "wn.synsets('koalas')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "teG92lto6AMv"
      },
      "source": [
        "We see above that the lexeme “koala” (whether represented by its singular or plural form) belongs to only one synset. The word “ghost”, though, belongs to seven, four of which are nouns and three of which are verbs:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c8Nyqgr16AMx",
        "outputId": "007c5566-fcbd-4bce-ee69-0e3a612e45a7"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Synset('ghost.n.01'),\n",
              " Synset('ghostwriter.n.01'),\n",
              " Synset('ghost.n.03'),\n",
              " Synset('touch.n.03'),\n",
              " Synset('ghost.v.01'),\n",
              " Synset('haunt.v.02'),\n",
              " Synset('ghost.v.03')]"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "wn.synsets('ghost')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xPTLZvhA6AMy"
      },
      "source": [
        "### Synset summary\n",
        "\n",
        "* A word may represent multiple meanings, and we get the meanings with `wn.synsets()`.\n",
        "* We can get a definition of a synset with `.definition()` .\n",
        "* We can get the lemmata (combination of a lexeme with a meaning) associated with a synset with `.lemmas()`.\n",
        "* We can get just the lexical part of a lemma with `.name()`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MV8f-hSv6AM0"
      },
      "source": [
        "## Using Wordnet to explore course project data\n",
        "\n",
        "For this tutorial, assume that we’re interested in words that express scary concepts. Assume that we’re interested in painful concepts instead of scary ones. The interesting words have already been tagged using manual methods, but we’re assuming that they are all tagged only in a simple way, along the lines of `<spooky_word>ghost</spooky_word>`. This initial markup makes it possible to find the words we care about easily, but it doesn’t tell us what they mean beyond the fact that they’re associated with scariness.\n",
        "\n",
        "We can begin our richer exploration of meaning by compiling a list of sample words and examining their synsets. In the example below we’ve included four spooky words plus one non-spooky control item:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JhBjfLLn6AM2",
        "outputId": "c35a62eb-4f65-4a95-e526-c16838f4475c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[Synset('panic.n.02'),\n",
              "  Synset('scare.n.02'),\n",
              "  Synset('frighten.v.01'),\n",
              "  Synset('daunt.v.01')],\n",
              " [Synset('ghost.n.01'),\n",
              "  Synset('ghostwriter.n.01'),\n",
              "  Synset('ghost.n.03'),\n",
              "  Synset('touch.n.03'),\n",
              "  Synset('ghost.v.01'),\n",
              "  Synset('haunt.v.02'),\n",
              "  Synset('ghost.v.03')],\n",
              " [Synset('fear.n.01'), Synset('frighten.v.01')],\n",
              " [Synset('creep.n.01'), Synset('ghost.n.01'), Synset('spook.v.01')],\n",
              " [Synset('koala.n.01')]]"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "from nltk.corpus import wordnet as wn # import Wordnet and call it just “wn” for brevity\n",
        "words = ['scare', 'ghost', 'fright', 'spook', 'koala'] # create a list of words to examine\n",
        "synset_list =[wn.synsets(word) for word in words] # get the synsets for each word\n",
        "synset_list # display them"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9A0gxgvX6AM4"
      },
      "source": [
        "The output above is a list of lists, where each of the inner lists contains the synsets that pertain to a particular word form. We can see that the first inner list shows the four synsets associated with the word “scare”, the second inner list shows the seven synsets associated with the word “ghost”, etc. Our assumption is that each word taken from a text is associated, _in the context in which it occurs_, with exactly one meaning represented by one of the available synsets. The part about context matters; the same lexeme may occur in different contexts with different meanings within the same text. For example, as noted above, the word “scare” may be a noun in one place and a verb in another.\n",
        "\n",
        "Occasionally your texts may contain words that are not included in Wordnet, or words that are used with meanings that are not represented in Wordnet. You cannot add anything to Wordnet, so when that happens, make a note of it, but otherwise you’ll have to exclude those words from your Wordnet processing."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z-kL8uhl6AM5"
      },
      "source": [
        "## Add synset markup to your documents\n",
        "\n",
        "So far your data contains nothing more than a tag that identifies spooky words, e.g., `<spooky_word>ghost</spooky_word>`. Your goal here is to identify the synset represented by the word in its context and add an attribute (`@synset`) to the markup, using a value that identifies the synset. This task requires human analysis, since although Wordnet can tell you the possible synsets for a particular lexeme, it can’t tell which of those available meanings the lexeme has at a particular location in the text. Remember that the same word form may represent different synsets in different locations. For example, as noted above, “scare” could be a noun in one place and a verb in a different place, and those are different synsets. You don’t need to do this for your entire corpus, which wouldn’t be realistic given the fifteen-week semester and the size of the corpus, but you’ll want to do enough to get a sense of the relationship between word forms in your corpus and the synsets that Wordnet uses to represent units of meaning.\n",
        "\n",
        "The procedure for adding synset markup to the document has three steps:\n",
        "\n",
        "1. Get the definitions of each synset for each scary word in your corpus or selection. You can use Python to do this.\n",
        "1. Choose the appropriate synset for each scary word in your corpus or selection. This requires human decisions, since Python doesn’t understand the context.\n",
        "1. Write the correct synset into the markup as a new `@synset` attribute. You have to do this manually, as well."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SFIuyxRV6AM7"
      },
      "source": [
        "### 1. Get the definitions of each synset for each word\n",
        "\n",
        "You can get the definition of a synset like `Synset('panic.n.02')` with:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "l382Icu16AM8",
        "outputId": "4bde55da-4f2a-4c06-c41e-65661e90ab76"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'sudden mass fear and anxiety over anticipated events'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "source": [
        "wn.synset('panic.n.02').definition()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h-V0Onlz6AM_"
      },
      "source": [
        "A lexeme like “scare” is associated with four synsets:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2ddlCnqQ6ANA",
        "outputId": "6a5b029e-332d-4917-b9dc-a4d5a0fbd049"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Synset('panic.n.02'),\n",
              " Synset('scare.n.02'),\n",
              " Synset('frighten.v.01'),\n",
              " Synset('daunt.v.01')]"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ],
      "source": [
        "wn.synsets('scare')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I24Yq-vj6ANA"
      },
      "source": [
        "For each occurrence of some form of “scare” in our texts (it might be ‘scare’ or ‘scares’ or some other inflected form), we want to add an attribute to our XML that indicates the appropriate synset. To tell the synsets apart (in case the sample word that’s part of the synset identifier is not sufficiently clear by itself), we can get their definitions. The code below outputs each synset and its definition:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9JPIZxcT6ANC",
        "outputId": "eaf52ef0-4d8c-4147-a0a3-6d60619a6cfa"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[\"Synset('panic.n.02') means: sudden mass fear and anxiety over anticipated events\",\n",
              " \"Synset('scare.n.02') means: a sudden attack of fear\",\n",
              " \"Synset('frighten.v.01') means: cause fear in\",\n",
              " \"Synset('daunt.v.01') means: cause to lose courage\"]"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ],
      "source": [
        "[str(item) + ' means: ' + item.definition() for item in wn.synsets('scare')]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eaQSJGTA6AND"
      },
      "source": [
        "We use the `str()` function above to stringify the synset (represented by the variable `item`) so that we can concatenate it with the other strings for output."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F4so91x86ANF"
      },
      "source": [
        "### 2. Choose the appropriate synset for each spooky word _in context_\n",
        "\n",
        "Once you know the synsets that are available for each word in your document, look at your XML and choose the appropriate synset for each word _in context_. For example, if “scare” occurs as a verb that means ‘cause fear in’ in one place, the synset you‘d choose from above would be 'frighten.v.01'. If it occurs as a noun that means ‘a sudden attack of fear’ in another, you‘d choose 'scare.n.02'."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g2wBaAAX6ANG"
      },
      "source": [
        "## Examine the lemmata for each synset\n",
        "\n",
        "At the moment this is just for curiosity. Below we construct a list of two synsets and for each of them we print the Wordnet synset identifier and a list of the lexemes associated with it. As described above, we use the `.lemmas()` method to get the lemmata associated with the synset and we use the `.name()` method to keep only the lexical part of the lemma:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cjuvQU2q6ANI",
        "outputId": "ef542a5a-5b64-4663-e555-7064d27307d8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Synset('scare.n.02') has the following lemmata: ['scare', 'panic_attack']\n",
            "Synset('frighten.v.01') has the following lemmata: ['frighten', 'fright', 'scare', 'affright']\n"
          ]
        }
      ],
      "source": [
        "scare_synsets = [wn.synset('scare.n.02'), wn.synset('frighten.v.01')]\n",
        "for synset in scare_synsets:\n",
        "    print(str(synset) + ' has the following lemmata: ' + str([lemma.name() for lemma in synset.lemmas()]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sYuA4nPw6ANK",
        "outputId": "49c20e86-9ed9-4244-85cb-c74a36113c88"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The word \"scare\" belongs to 4 synsets\n",
            "The word \"ghost\" belongs to 7 synsets\n",
            "The word \"fright\" belongs to 2 synsets\n",
            "The word \"spook\" belongs to 3 synsets\n",
            "The word \"koala\" belongs to 1 synsets\n"
          ]
        }
      ],
      "source": [
        "for word in words:\n",
        "    synset_count = len(wn.synsets(word))\n",
        "    print('The word \"' + word + '\" belongs to ' + str(synset_count) + ' synsets')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 237
        },
        "id": "AYE7EJuh6ANM",
        "outputId": "e42b4c45-1546-449c-f80b-d1ec209dce4d"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-19-1bfa7e842139>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'spooky_words.txt'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0minfile\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m# open the plain text file that contains the list of words\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mwordlist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# read the words into a list, splitting on the new lines\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'synset_counts.xml'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'w'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0moutfile\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m# open a file to hold the XML output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0moutfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'<root>'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# create a start tag for the root element in the output XML file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mwordlist\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m# create output for each word\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'spooky_words.txt'"
          ]
        }
      ],
      "source": [
        "with open('spooky_words.txt', 'r') as infile: # open the plain text file that contains the list of words\n",
        "    wordlist = infile.read().split() # read the words into a list, splitting on the new lines\n",
        "with open('synset_counts.xml', 'w') as outfile: # open a file to hold the XML output\n",
        "    outfile.write('<root>') # create a start tag for the root element in the output XML file\n",
        "    for word in wordlist: # create output for each word\n",
        "        synset_count = len(wn.synsets(word)) # for each word, count the number of synsets to which it belongs\n",
        "        outfile.write('<word><form>' + word + '</form><count>' + str(synset_count) + '</count></word>') # write it out\n",
        "    outfile.write('</root>') # create the end tag for the root element"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fNaYKOeY6ANM"
      },
      "source": [
        "We saved the output to a file called synset\\_counts.xml, so we don’t see it here in the notebook, but we can now use Python to read it. This is just for human inspection, to make sure that it looks the way we want. It isn’t pretty-printed, but we can still see how it looks:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6iB9JCEq6ANN",
        "outputId": "6c9c6735-8b90-4bcc-f90e-17156946ee0a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<root><word><form>ghost</form><count>7</count></word><word><form>scared</form><count>4</count></word><word><form>scare</form><count>4</count></word></root>\n"
          ]
        }
      ],
      "source": [
        "with open('synset_counts.xml') as infile:\n",
        "    print(infile.read())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SvX-PZv26ANP"
      },
      "source": [
        "### Explore the richness of the vocabulary (by writer or by text)\n",
        "\n",
        "Synsets are represented by one or more lemmata, which you can retrieve with the `lemmas()` method, as in:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YSsTm_Dl6ANQ",
        "outputId": "6cd54917-120e-4c77-d331-ac6c389acf83"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Synset('ghost.n.01') means \"a mental representation of some haunting experience\" and has 6 lemmata: ['ghost', 'shade', 'spook', 'wraith', 'specter', 'spectre']\n",
            "Synset('ghostwriter.n.01') means \"a writer who gives the credit of authorship to someone else\" and has 2 lemmata: ['ghostwriter', 'ghost']\n",
            "Synset('ghost.n.03') means \"the visible disembodied soul of a dead person\" and has 1 lemmata: ['ghost']\n",
            "Synset('touch.n.03') means \"a suggestion of some quality\" and has 3 lemmata: ['touch', 'trace', 'ghost']\n",
            "Synset('ghost.v.01') means \"move like a ghost\" and has 1 lemmata: ['ghost']\n",
            "Synset('haunt.v.02') means \"haunt like a ghost; pursue\" and has 3 lemmata: ['haunt', 'obsess', 'ghost']\n",
            "Synset('ghost.v.03') means \"write for someone else\" and has 2 lemmata: ['ghost', 'ghostwrite']\n"
          ]
        }
      ],
      "source": [
        "synsets = wn.synsets('ghost')\n",
        "for synset in synsets:\n",
        "    lemmata = synset.lemmas()\n",
        "    print(str(synset) + ' means \"' + synset.definition() + '\" and has ' + str(len(lemmata)) + ' lemmata: ' + \\\n",
        "         str([lemma.name() for lemma in lemmata]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nQAd56nZ6ANS"
      },
      "source": [
        "We use the `name()` method to get just the lexical part of the lemma. (We took a lazy way out and used the plural “lemmata” even after the value 1, although “has 1 lemmata” should really read “has 1 lemma”. If we intended to use this code to produce final output for end-users, we’d include additional code to control for that difference.)\n",
        "\n",
        "A writer or text that uses the synset 'ghost.n.01' has six lemmata available to express that meaning. What proportion of the available vocabulary does your writer or text use? \n",
        "\n",
        "That would be easy to calculate if the writer always used the exact form provided by the `name()` method of lemmata. For example, you might find that a particular text contains the following mappings of lemmata and word forms:\n",
        "\n",
        "Synset | Word form\n",
        "--- | ---\n",
        "ghost.n.01 | ghost\n",
        "ghost.n.01 | shade\n",
        "ghost.n.01 | spook\n",
        "\n",
        "You can count up the number of word forms associated with each synset, and because each word form corresponds to a different one of the 6 lemmata for that synset, you’ll determine correctly that the writer or text uses 50% of the available lemmata."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NUcAVBW16ANU",
        "outputId": "aba33eed-5b88-4c6a-bccc-63c869ee411c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "There are 6 lemmata for ghost.n.01 and they are: ['ghost', 'shade', 'spook', 'wraith', 'specter', 'spectre']\n",
            "The 3 lemmata for ghost.n.01 used in the document are ['ghost', 'shade', 'spook']\n",
            "The ratio of used (3) divided by available (6) = 0.5\n"
          ]
        }
      ],
      "source": [
        "available = [lemma.name() for lemma in wn.synset('ghost.n.01').lemmas()]\n",
        "print('There are ' + str(len(available)) + ' lemmata for ghost.n.01 and they are: ' + str(available))\n",
        "used = ['ghost', 'shade', 'spook']\n",
        "print('The 3 lemmata for ghost.n.01 used in the document are ' + str(used))\n",
        "print('The ratio of used (' + str(len(used)) + ') divided by available (' + \\\n",
        "      str(len(available)) + ') = ' + str(len(used) / len(available)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j2FIs_CO6ANV"
      },
      "source": [
        "But suppose the forms include different inflections of the same lemma, such as singular 'ghost' and plural 'ghosts'. The challenge here is that those two forms represent the same lemma, and the `.lemmas()` won’t return the plural form “ghosts”, so you can’t simply count forms that occur in the text and use that as a surrogate for counting lemmata that occur in the text. Wordnet helps resolve these situations with `wn.morphy()`, which lemmatizes:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LEUKRdJe6ANY",
        "outputId": "cb820822-5498-4a6a-f098-39d561f6c00a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The result of applying wn.morphy() to “ghost” (sg) is “ghost”\n",
            "The result of applying wn.morphy() to “ghosts” (pl) is ghost”\n"
          ]
        }
      ],
      "source": [
        "print('The result of applying wn.morphy() to “ghost” (sg) is “' + wn.morphy('ghost') + '”')\n",
        "print('The result of applying wn.morphy() to “ghosts” (pl) is ' + wn.morphy('ghosts') + '”')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vhc-D96x6ANa"
      },
      "source": [
        "This means that we can resolve the variation caused by inflection along the following lines. In the code snippet below we have the same list as above, except that instead of three items in our `used` variable that correspond to three different lemmata, we have three items that correspond to only two different lemmata. Here we print the values of `used` and `normalized` to show that they have the same length (the same number of items), but `normalized` has only two _distinct_ values, while `used` has three. We then convert `normalized` from a list (which allows duplicates) to a set (which doesn’t), which is a quick way of removing duplicates:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cHDUHaCA6ANb",
        "outputId": "d120d9eb-9896-4efd-fdde-fac315fcdb5f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['ghost', 'ghosts', 'spook']\n",
            "['ghost', 'ghost', 'spook']\n",
            "0.3333333333333333\n"
          ]
        }
      ],
      "source": [
        "available = [lemma.name() for lemma in wn.synset('ghost.n.01').lemmas()]\n",
        "used = ['ghost', 'ghosts', 'spook']\n",
        "normalized = [wn.morphy(item) for item in used]\n",
        "print(used)\n",
        "print(normalized)\n",
        "print(len(set(normalized)) / len(available))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mBLupGgS6ANc"
      },
      "source": [
        "By using `wn.morphy()`, then, we can determine the richness of the vocabulary (the number of available different lemmata that are actually used) without being misled by different inflected forms of the same lexeme. Of course we still have to decide how to use these counts to explore or present information about how much of the available vocabulary variation the writer or text actually uses."
      ]
    }
  ],
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.5"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}